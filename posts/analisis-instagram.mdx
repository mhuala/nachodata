---
id: 3
title: Analisis túrismo basado en Instagram
author: Nacho Huala
time: "11"
date: 01/08/2023
tags: [pandas,seaborn, python]
description: ¿Pueden las publicaciones de Instagram ser usadas para analizar la actividad turística de una locación?
bannerURL: https://webunwto.s3.eu-west-1.amazonaws.com/2021-11/unwto-and-instagram-partner-to-help-destinations-recover-and-rediscover_1.jpg
---

# Dé que se trata?

En los ultimos años ha sido de bastante interés

-
-

-
-

-
-

-

Ampliamente estudiado y analizado por distintos medios e instituciones.

En Chile aún se está al debe respecto a analisis en esta materia, lo único cercano a un análisis basado en datos de RRSS es lo realizado por el
["Observatorio Turístico BIG DATA - Región Metropolitana"](http://dataturismorm.cl/demanda-turistica.html) en dónde se menciona en una visualización de valores obtenidos de analisis de sentimiento sin mayores detalles que:

"Las visitas a los principales atractivos de la Región Metropolitana, es monitoreado en redes sociales a través de las publicaciones de los usuarios."

Años atrás traté de proponer (fallidamente) como tema de tesis algo relacionado al análisis de datos de publicaciones realizadas en Valdivia, antes de siquiera proponerlo en la etapa inicial pensaba que era importante demostrar alguna correlación de algún tipo entre el comportamiento de las publicaciones y alguna variable de interés, las que seleccioné fueron las siguientes:

-   Tendencias de Google Trends
-   X

# Obtención de datos

El cómo fueron obtenidos estos datos, no tiene una explicación relevante puesto que los terminos de condiciones de Instagram así como las barreras de seguridad interpuestas por la misma compañía a la hora de comunicarse con sus servidores ha cambiado drásticamente desde 2019.

<ButtonUrl href="https://github.com/mhuala/Instagram-posts-on-tourism-prediction-Valdivia/tree/main/Datos">Datos utilizados</ButtonUrl>

## Por qué es tan dificil scrapear masivamente Instagram en la actualidad ?

Hasta los años 2019-2020 aún era posible extraer grandes volumenes de datos de Instagram, bastaba con realizar requests a los endpoints publicos que existían e ir iterando de alguna forma de "página a página", no era exactamente eso pero la lógica no variaba mucho.

Existían varios videos inclusive en Youtube que explicaban el cómo se debía hacer esto, videos que quedaron descontinuados al momento en que Instagram, buscando resguardar más sus datos, actualiza sus normas de seguridad.

-   Una nueva API la cual está enlazada con la API de Facebook y es bastante más restrictiva que la API antigua.
-   Se requiere de autenticación para acceder al sitio desde IPs de datacenters.
-   Se requiere de autenticación para acceder al sitio desde IPs residenciales luego de unas cuantas peticiones/visitas.

# Análisis de publicaciones:

Una vez descargados los datos del repositorio (disponibles en la sección de *material extra*), se puede empezar el análisis que parte con la importación de librerías necesarias.

<Code language="python">
import pandas as pd
from numpy.random import randn
import matplotlib.pyplot as plt
import seaborn as sns
import json
import calendar
from datetime import datetime, timedelta
</Code>

<Code language="python">
df_imacec = pd.read_csv('../Datos/IMACEC/IMACEC.csv')
df_gt = pd.read_csv('../Datos/Google Trends/Valdivia-GoogleTrends.csv')
df_dolar = pd.read_csv('../Datos/Dolar Historico/Dolar_2014-2020.csv')
df_clima = pd.read_json('../Datos/Clima/Valdivia_weather.json')
df_emat = pd.read_csv('../Datos/Encuesta Mensual de Alojamiento Turistico (EMAT)/emat_los_rios.csv')
df_ig = pd.read_csv('../Datos/Posts/valdivia_posts.csv')
</Code>

## IMACEC

Para limpiar estos datos se debe usar la función apply() y dentro de esta utilizar las funciones datetime.strptime() y strftime() de esta manera se convierten las fechas que por defecto tienen el siguiente formato DD/MM/YYYY a uno de la forma YYYY-MM-DD que sea psoible trabajar con los metodos que posee un dato de tipo datetime

Finalmente se seleccionan todos los datos a partir del año 2014.

<Code language="python">
df_imacec["date"]= df_imacec.date.apply(lambda row: datetime.strptime(row, "%d/%m/%Y").strftime("%Y-%m-%d"))
df_imacec["date"]= pd.to_datetime(df_imacec.date)
df_imacec = df_imacec.loc[df_imacec['date'] >= '2014']
</Code>

## Google Trends

Para limpiar estos datos se debe usar la función to_datetime() , debido a que este dataframe no posee los días y por defecto la función anterior retornará la fecha en el primer día de cada mes. Para arreglar esto y sumar a la fecha los días correspondientes en cada mes para que cada fecha quede en el ultimo día de su respectivo mes se puede hacer lo siguiente:

-   Utilizar el metodo apply() y dentro de el realizar la suma de las fechas por fila
-   Utilizar el metodo calendar.monthrange() para encontrar el último día de cada mes y además timedelta() para ir sumando esa cantidad de días a la fila actual.
-   Se cambian los nombres de las columnas para facilitar su manejo.

## Dolar histórico

Para limpiar estos datos se debe usar la función to_datetime() , para convertir la columna de string a datetime

-   Se utiliza el metodo resample() en conjunto al metodo mean() de esta forma se obtiene el promedio del dolar en cada mes
-   Utilizar el metodo calendar.monthrange() para encontrar el último día de cada mes y además timedelta() para ir sumando esa cantidad de días a la fila actual.

Finalmente se crea un nuevo dataframe con la serie obtenida con el metodo anterior.

## Clima histórico

Para limpiar estos datos se debe usar la función to_datetime() , para convertir la columna de string a datetime

-   Se utliza el metodo resample() en conjunto al metodo mean() de esta forma se obtiene el promedio del dolar en cada mes
-   Se utiliza el metodo calendar.monthrange() para encontrar el último día de cada mes y además timedelta() para ir sumando esa cantidad de días a la fila actual.

Finalmente este dataframe posee las siguientes variables :

-   TP : Temperatura promedio
-   TMAX : Temperatura Maxima
-   TMIN : Temperatura Minima
-   P_ATM: Presión Atmosferica
-   HUM: Humedad
-   PP: Precipitaciones
-   VISI : Visibilidad
-   VV : Velocidad Viento

## EMAT

Para limpiar estos datos se usa la función to_datetime(), para convertir la columna de string a datetime

-   Se utiliza el metodo resample() en conjunto al metodo mean() de esta forma se obtiene el promedio del dolar en cada mes
-   Se utiliza el metodo calendar.monthrange() para encontrar el último día de cada mes y además timedelta() para ir sumando esa cantidad de días a la fila actual.

Finalmente este dataframe posee las siguientes variables :

-   LL_CH : Llegadas de chilenos a la Región de los Ríos
-   LL_EXT : Llegadas de extranjeros a la Región de los Ríos
-   LL_TOT: Llegadas totales a la Región de los Ríos
-   PER_CH: Pernoctaciones de chilenos en la Región de los Ríos
-   PER_EXT: Pernoctaciones de extranjeros en la Región de los Ríos
-   PER_TOT: Pernoctaciones totales en la Región de los Ríos

## Instagram

Este dataframe viene listo en formato, las variables son las siguientes:

-   user_id : ID unica de usuario de Instagram.
-   date : Fecha de publicación.
-   shortcode : Identificador unico de publicación.
-   post_text: Texto que acompaña a la publicación
-   post_reactions: Cantidad de Likes de la publicación.
-   post_is_video: Booleano que es True en caso de que la publicación sea un video, en vez de una foto.

Debido a la gran cantidad de datos se puede verificar que no hayan nulos ni repetidos usando el metodo describe(include='all'), la idea es tener registros unicos evitando algún tipo de sesgo que se pueda generar por tener datos repetidos o nulos.

El identificador unico de cada publicación es el shortcode y se puede observar que del total no todos son unicos, se debe eliminar los duplicados para esto se utiliza el metodo drop_duplicates(subset = ['shortcode']). Finalmente se verifica con describe(include='all').

<Code language="python">
df_ig = df_ig.drop_duplicates(subset = ['shortcode'])
df_ig.describe(include='all')
</Code>

Se crea una columna de valores "1", esto permite realizar la suma por fecha posteriormente. También a partir de la columna de booleanos post_is_video se crean nuevas columnas de variables dummies con get_dummies(df_ig["post_is_video"]) que valdrán 1 ó 0 dependiendo si son o no videos.

<Code language="python">
df_ig["post_count"] = 1
dummy = pd.get_dummies(df_ig["post_is_video"])
dummy.head()
</Code>

Para poder sumar por fecha la cantidad de posts, posts que son fotos y posts que son videos se puede usar la función resample() en conjunto con el metodo sum() sobre la columna post_count. Este valor se guarda en una variable que luego se puede plotear en una serie de tiempo

<Code language="python">
df_ig["date"] = pd.to_datetime(df_ig["date"])
posts_sum = df_ig.resample('M', on='date')['post_count'].sum()
time_serie = pd.DataFrame(data=posts_sum)
time_serie = time_serie.reset_index()
</Code>

Para poder crear un nuevo dataframe que tenga por columna los meses, por indices los años y por valores la cantidad de publicaciónes en cada mes de cada año , se crea una función llamadamont_post_recorder() la cual retornará una lista que contiene los valores de todos los años para el mismo mes.

<Code language="python">
def month_post_recorder(month):
    my_list = []
    time_serie.loc[(time_serie['date'].dt.month==month)]['post_count'].apply(lambda row: my_list.append(row))
    return my_list
</Code>

Se crea nuevo dataframe con los valores para cada mes, finalmente se deja la columna Years como Indice.

<Code language="python">
years_list = [x for x in range(2012,2021)]
d = {'Year': years_list ,
     'Jan': month_post_recorder(1) ,
     'Feb': month_post_recorder(2) ,
     'Mar': month_post_recorder(3) ,
     'Apr': month_post_recorder(4) ,
     'May': month_post_recorder(5) ,
     'Jun': month_post_recorder(6) ,
     'Jul': month_post_recorder(7) ,
     'Aug': month_post_recorder(8) ,
     'Sep': month_post_recorder(9) ,
     'Oct': month_post_recorder(10) ,
     'Nov': month_post_recorder(11) ,
     'Dec': month_post_recorder(12) }

post_per_month = pd.DataFrame(data=d)
post_per_month = post_per_month.set_index('Year')
</Code>

Con la librería Seaborn se puede grafucar la serie de tiempo que muestra las publicaciones que tienen geo-etiquetado a Valdivia que se emiten por mes y también el gráfico que muestra como han incremententado por mes las publicaciones a lo largo de los años.

<Code language="python">
fig,axes=plt.subplots(1,2, figsize=(15, 5))
# 
with sns.axes_style("darkgrid"):
    sns.lineplot(data=post_per_month,ax=axes[1])
    axes[1].set_title('Publicaciones de cada mes por año')
    axes[1].grid(True)
    sns.lineplot(data=time_serie, x="date", y="post_count", ax=axes[0])
    axes[0].set_title('Publicaciones a lo largo del tiempo')
    axes[0].grid(True)
</Code>

<Photo postId="3" fileName="1"/>

Con el fin de tratar de inferir quienes de las personas que publican son turistas que visitan la Región, se propone la siguiente solución.

Al fin de estimar el rango de tiempo que está un turista extranjero en Chile se indaga en los datos otorgados por la Subsecretaría de Turismo en la planilla llamada Gasto, Permanencia Promedio e Ingreso de Divisas de los turistas extranjeros que visitan Chile. Serie 2001-2016 y vemos que no pasa más allá de 14 días como promedio máximo a lo largo de los años .

Para obtener este dato de los turistas nacionales se puede ver en Perfil de turistas nacionales de Sernatur que muestra un promedio para viajes largos (dónde uno de los destinos se especifica a Valdivia y Corral) es de 9.7 días.

Teniendo estos datos se puede generalizar y decir que el criterio que se usará para saber si cada publicación es hecha por un turista (sea extranjero o nacional) es que : Haya un lapso maximo de 14 días entre la primera y última publicación de este usuario con esta geo-etiqueta

Esto entendiendo la lógica de inmediatez de Instagram además de que probablemente nadie salga de vacaciones y espere más de 14 días para publicar sus fotos, finalmente de esta forma evitamos cuentas pertenecientes a tiendas o emprendimientos que pudieran estar Spammeando publicaciones con esta geo-etiqueta.

Se agrupan a los usuarios por sus fechas con el criterio de fecha minima (la fecha mas antigua) y fecha máxima (fecha mas reciente)con groupby('user_id')['date'] y aplicando este criterio posteriormente con agg(['min','max']). Finalmente se cambia el nombre de las columnas generadas por este agrupamiento para que sean mas descriptivas. Posteriormente se restan ambas fechas y así se obtiene el numero de días que hay entre la primera y última publicación, recordando que esto ibamos a usar cómo criterio para definir quién era turista y quién no.

<Code language="python">
first_and_last = df_ig.groupby('user_id')['date'].agg(['min','max']).rename(columns={'min':'first','max':'last'})
first_and_last["diff_days"] = (first_and_last['last'] - first_and_last['first']).dt.days
</Code>

Usando la librería Seaborn se puede ver como se distribuyen los días de diferencia entre la primera y ultima publicación para los distintos usuarios.

<Code language="python">
fig,axes=plt.subplots(1,2, figsize=(15, 5))
#Grafico 1
sns.distplot(first_and_last['diff_days'],kde = True,rug=True, color = 'darkblue', hist_kws={'edgecolor':'black'},kde_kws={'linewidth': 3},ax=axes[0])
#Grafico 2
sns.distplot(first_and_last['diff_days'], hist=True, kde=False, color = 'darkblue', hist_kws={'edgecolor':'black'},kde_kws={'linewidth': 3},bins=50,ax = axes[1])
# Configuraciones del gráfico
plt.grid(True)
plt.xlim(0, 500)
# Configuración de titulos
axes[0].set_title('Días entre primera y última publicación (Densidad)')
axes[0].grid(True)
axes[1].set_title('Días entre primera y última publicación (Histograma Zoom-In)')
axes[1].grid(True)
# Creación de la imagen
plt.show()
</Code>

<Photo postId="3" fileName="2"/>

Se puede ver el promedio de publicaciones por personas, dividiendo el total de registros unicos del primer df_ig luego de eliminar los duplicados , con el de cualquier columna del dataframe first_and_last (que representa la cantidad de usuarios unicos que hay entre todas las publicaciones), finalmente se divide 1 en ese cociente.

<Code language="python">
r_1 =len(first_and_last.index)/ len(df_ig["user_id"])
prom = 1/r_1
print(f"Promedio:{prom}") 
#Promedio: 5.423
</Code>
Ahora que se tiene un dataframe con la diferencia de días que tiene cada usuario entre su primera y ultima publicación, se puede crear una mascara (mask) que permita "filtrar" y solo queden los usuarios que tienen una diferencía de días entre su primera y ultima publicaicón menor o igual a 14 días que fue lo planteado anteriormente.

<Code language="python">
mask = (first_and_last['diff_days'] <= 14)
first_and_last_tourist = first_and_last[mask]
r_2 = first_and_last_tourist.shape[0]/len(first_and_last.index)*100
print(f"Porcentaje de clasisifados como turistas : {r_2}%")
#r_2 = 69.445%
</Code>

Ya teniendo un dataframe solo con usuarios que cumplen este requisito de Turismo se procede a crear una columna con un 1, para representar que hay 1 turista en cada registro y luego agruparlos por mes (tal como se ha trabajado todos los otros dataframes) finalmente sumandolos.

<Code language="python">
first_and_last_tourist['post_count']=1
sf = first_and_last_tourist.resample('M', on='first')['post_count'].sum()
time_serie_tourists = pd.DataFrame({'date':sf.index, 'total':sf.values})
</Code>

Al visualizar estos datos se puede ver como al aplicar este filtro el comportamiento de la serie de tiempo cambia drasticamente y toma un caracter mas estacionario , como la actividad turistica.

<Code language="python">
fig,axes=plt.subplots(1,2, figsize=(15, 5))
# Creación de gráfico
with sns.axes_style("darkgrid"):
    sns.lineplot(data=time_serie_tourists, x="date",y="total",ax=axes[1])
    axes[1].set_title('Publicaciones a lo largo del tiempo "TURISTAS"')
    axes[1].grid(True)
    sns.lineplot(data=time_serie, x="date", y="post_count", ax=axes[0])
    axes[0].set_title('Publicaciones a lo largo del tiempo TOTAL')
    axes[0].grid(True)
</Code>

<Photo postId="3" fileName="4"/>

Ahora ya teniendo todos los datos limpios y en un mismo formato de fechas, se puede crear un dataframe final el cual contendrá la información de todos los dataframes cargados inicialmente además de la información sobre las publicaciones a lo largo del tiempo con el filtro de los 14 días para Turistas.

Primero se crea una función llamadacut_dates() que permitirá obtener los datos comprendidos entre 2014 y 2020 de cada dataframe inicial.

<Code language="python">
def cut_dates(df):
    df =df.loc[df['date']>='2014']
    df =df.loc[df['date']<'2020']
    return df 
</Code>

Posteriormente se aplica la función a los dataframes iniciales, para finalmente utilizando merge() poder unirlos usando como unión la fila de las fechas creando un dataframe final sobre el cual se analizará si existen correlaciones lineales.

<Code language="python">
df_imacec = cut_dates(df_imacec)
time_serie_tourists = cut_dates(time_serie_tourists)
df_usd = cut_dates(df_usd)
df_gt = cut_dates(df_gt)
df_emat = cut_dates(df_emat)
df_clima = cut_dates(df_clima)
df_list = [df_imacec,time_serie_tourists,df_usd,df_gt,df_emat,df_clima]
</Code>

<Code language="python">
final = pd.merge(df_imacec, time_serie_tourists, how='outer', on='date')
final = pd.merge(final, df_usd, how='outer', on='date')
final = pd.merge(final, df_gt, how='outer', on='date')
final = pd.merge(final, df_emat, how='outer', on='date')
final = pd.merge(final, df_clima, how='outer', on='date')
final.rename(columns={'total_x':'tourists_posts','total_y':'dolar_price'} , inplace=True)
</Code>

En este punto con el DataFrame final listo, ya se puede analizar si existe una correlación entre los datos de la EMAT: Encuesta Mensual de Alojamiento Túristico con la cantidad de publicaciones generadas en Instagram sobre un lugar y las tendencias de busqueda en Google. Para ello aplicamos el siguiente código.

<Code language="python">
fig, ax = plt.subplots(figsize=(15,15)) 
sns.heatmap(final.corr(), annot = True, vmin=-1, vmax=1, center= 0,linewidths=.5, ax=ax)
</Code>

<Photo postId="3" fileName="5"/>

Se pueden ver las correlaciones de las variables de llegadas totales (LL_TOT) y pernoctaciones totales (PER_TOT) presentes en la EMAT con otras variables como :
*   LL_TOT: TMAX = 0.61 , TP = 0.58, google_trends = 0.73, tourists_posts = 0.67
*   PER_TOT:TMAX = 0.61 , TP = 0.56, google_trends = 0.72, tourists_posts = 0.68

<Extra>
Otro material que puede ser de ayuda:

<ButtonUrl href="https://estadisticas.sed.srcei.cl" callToAction="Página registro civil"/>
<ButtonUrl href="https://playwright.dev/python/docs/intro" callToAction="Documentación Playwright"/>

El repositorio con todos los archivos puede ser encontrado en:

<Repository repoName="Instagram-posts-on-tourism-prediction-Valdivia"/>
</Extra>